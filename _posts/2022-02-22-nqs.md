---
layout: post
title: Harnessing NQS for Quantum Many-Body Systems
tags: ["machine learning", "quantum physics", "ising model"]
date: 2022-02-22 23:52:05 +0800
math: true
---

### Introduction
Quantum many-body physics looks at systems with many fundamental particles at the scale of atoms and below. The information about the dynamics of such systems is stored in the wave function. However, the number of states of such a system grows exponentially in the number of particles in the system. This makes the study of such systems extremely hard, as they require a large amount of memory to store the entire wavefunction, presenting formidable computational challenges. 

Traditional methods often struggle with scalability, which has opened the door for machine learning techniques to offer innovative solutions. This post explores how Neural-network Quantum States (NQS), specifically leveraging Restricted Boltzmann Machines (RBMs) and Multilayer Perceptrons (MLPs), are applied to study the ground and excited states of the one-dimensional Ising $J_1-J_2$ model. This work was part of my master's thesis.

### The Quantum Many-Body Problem
The behavior of a quantum many-body system, consisting of interacting particles, is captured in its wave function, $\ket{\Psi}$. A critical task in quantum mechanics is solving Schrödinger’s equation:

$$ H \ket{\Psi_i} = E_i \ket{\Psi_i}$$

Here, $H$ is the Hamiltonian matrix, whose eigenvalues $E_i$ represent the system's energies, and $\ket{\Psi_i}$ are the corresponding states. Typically, we are interested in finding the ground state energy $E_0$ and state $\ket{\Psi_0}$. This is because at the ground state, the effects of quantum mechanics are more pronounced and easier to observe, as compared to excited
states.

#### The Ising $J_1-J_2$ model
The Ising $J_1-J_2$ model introduces complexities by incorporating both nearest-neighbor ($J_1$) and next-nearest-neighbor ($J_2$) interactions, in the presence of an external magnetic field, $h$. 

| <img src="{{site.url}}{{site.baseurl}}/assets/img/J1J2.png" alt="Nearest and Next-Nearest Neighbor interactions" width="1000"/> | 
|:--:| 
| *Simplified one dimensional model depicting nearest and next nearest neighbor interactions* |

Depending on the values these two parameters take (in addition to the value of the external magnetic field), the system exhibits distinct magnetic phases:

- When $J_1 < 0$ and $J_2 \gg J_1 / 2$ and both $J_1$ and $J_2$ dominate $h$, the nearest neighbors want to be aligned anti-parallel to each other due to $J_1$ and the next-nearest neighbors want to be aligned parallel to each other because of $J_2$, making $\ket{\uparrow\downarrow\uparrow\downarrow\dots}$ and $\ket{\downarrow\uparrow\downarrow\uparrow\dots}$ the most probable (spin) configurations. Therefore, the system is in the _nearest neighbor antiferromagnetic phase_.
    
- When $J_1 > 0$ and $J_2 \gg J_1 / 2$ and both $J_1$ and $J_2$ dominate $h$, the nearest and next-nearest neighbors want to align parallel to each other, making $\ket{\uparrow\uparrow\dots\uparrow}$ and $\ket{\downarrow\downarrow\dots\downarrow}$ as the most probable (spin) configurations. Therefore, the system is in the _ferromagnetic phase_.
    
- when $J_2 \ll 0$ and $mod(J_2) > mod(J_1) / 2$ and both $J_1$ and $J_2$ dominate $h$, the next-nearest neighbors want to be aligned anti-parallel to each other because of $J_2$, while the nearest neighbors want to be aligned parallel (if $J_1>0$) or anti-parallel (if $J_1<0$). This makes $\ket{\uparrow\uparrow\downarrow\downarrow\dots}$, $\ket{\downarrow\downarrow\uparrow\uparrow\dots}$, $\ket{\downarrow\uparrow\uparrow\downarrow\dots}$ and $\ket{\uparrow\downarrow\downarrow\uparrow\dots}$ the most probable configurations. Therefore, the system is in the _next-nearest neighbour antiferromagnetic phase_.

<!-- However, since we were studying a system with open boundary conditions, the ground state of the system in the next-nearest neighbour antiferromagnetic phase depends on the sign of $J_1$. When $J_1 < 0$, the two most probable states in the ground state are $|\uparrow\downarrow\downarrow\uparrow \dots \rangle$ and $|\downarrow\uparrow\uparrow\downarrow \dots \rangle$. When $J_1 > 0$, the two most probable states in the ground state are $|\uparrow\uparrow\downarrow\downarrow \dots \rangle$ and $|\downarrow\downarrow\uparrow\uparrow  \dots \rangle$. When $J_1 = 0$, the four states are the most probable and equiprobable. -->

### Neural-network Quantum States: A Machine Learning Approach

Neural-network Quantum States (NQS) utilize neural networks to approximate wave functions. The NQS approach is particularly advantageous due to:

- Adaptability: NQS have been used to successfully model different quantum many-body systems (Ising, Ising $J_1-J_2$, Heisenberg,...). As part of this thesis, they were used to model both ground and excited states.

- Efficiency: They exploit hardware and software advancements in machine learning.

- Effectiveness: They often match traditional computational methods.

The two architectures explored in this study are:

- Restricted Boltzmann Machines (RBM): Generative energy-based models with a single hidden layer. They are adept at unsupervised learning of probability distributions.

- Multilayer Perceptrons (MLP): Feedforward networks with one hidden layer in our experiments, using `ReLU` activations for the ground state and `tanh` for excited states.

#### Transfer Learning for Ground States

To address scalability, transfer learning protocols were employed. We first train a neural network (_base network_) for a task (_base task_); then we train another neural network (_target network_) for another similar task (_target task_).
For the transfer, we initialise the target network with the parameters of the base network, instead of a random set. The former is called a _hot-start_ while the latter (i.e., randomly
initialised) is called a _cold-start_. After the transfer, the target network is further trained and
this step is called _fine-tuning the network_ for the target task. 

As one can imagine, there are several issues that arise when naively transferring parameters from one
network to another. Typically, the base and target networks have the same architecture, leaving no flexibility in changing the structure of the target network. When the input dimensions do not match for the two networks, we project or embed the input of the target task such that its dimensions match that of the base task (such as image resizing in computer
vision tasks). This is where a tiling method proposed by Zen et al. was employed and evaluated. Different $(k,p)-$ tiling approaches were used to initialize groups of weights, repeated as we went from small to larger systems.

| <img src="{{site.url}}{{site.baseurl}}/assets/img/kp-tiling.png" alt="kp-tiling visual" width="1000"/> | 
|:--:| 
| *Schematic representation of the different transfer learning protocols used to scale up one-dimensional systems.* |

$N, N_h$ and $N', N'_h$ correspond to the number of visible and hidden nodes for the base and target networks, respectively. The different panels show how to construct the target weight matrix $\mathbf{W}$ from the base weight matrix $\tilde{\mathbf{W}}$ by replicating the colored rows and filling the grey cells with randoms entries. Panels ${(a)}, {(b)} and {(c)}$: $(1,2)-tiling$, $(2,2)-tiling$ and $(L,2)-tiling$, respectively. Here the scaling factor is 2. Panel ${(d)}$ shows the $(L,4)-tiling$ used when scaling up the network by a factor 4.


**Results** indicated that $(L,2)$-tiling (full weight reuse, twice repeated) is most effective and efficient across all magnetic phases.


### Performance Evaluation

In order to evlauate the hypothesis, we relied on two metrics:
- Efficiency: Time to convergence
- Effectiveness: Relative energy error compared to exact methods (the classical computational methods)

### Findings:

The best performing transfer learning protocol is the $(L, 2)−tiling$ protocol, which was effective across the different phases for all the system sizes we considered. 

The experiments demonstrated that employing transfer learning for the scalability of NQS (when determining ground states) can be both:
- efficient: provides shorter convergence times, in comparison to cold-start runs, particularly for larger systems
- effective: reduces the chances of the optimization being stuck in a local minima by initialising from a converged base network

Notably, the choice of tiling protocol is quite critical. If the characteristic traits of the target ground state phase are not preserved in the transfer
learning protocol, the transfer learning method becomes ineffective and inefficient. On the other hand, protocols that preserve such traits (such as $(L, 2) − tiling$) are generally stable.

In addition to the ground state, some time was also spent evaluating NQS for finding low-lying excited states. The reader is referred to the thesis report for those. 

### Conclusion
Neural-network Quantum States offer a promising avenue for tackling quantum many-body problems. By employing transfer learning protocols like, we can significantly enhance the scalability of NQS for studying the Ising $J_1-J_2$ model. 

There are several lines of enquiry possible from here. THe direction of transfer learning for this study was from small to larger systems. The opposite direction might also be interesting, as it would require devising protocols that marginalise or truncate the parameters of the NQS, as opposed to the existing ones that replicate the parameters. In the transfer learning context, transferring across different models could also be considered - between models of different dimensions or completely different models as well. 


